{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Логистическая регрессия</center>\n",
    "\n",
    "#### Линейный классификатор\n",
    "\n",
    "Перед тем, как непосредственно перейти к логистической регрессии, рассмотрим задачу линейной классификации в целом. И начнем с самого простого ее варианта - бинарной классификации. В этом случае мы будем определять к какому из двух классов можно отнести объект.  \n",
    "  \n",
    "Мы уже знакомы с линейной регрессией и линейный классификатор устроен очень похоже. В линейной регрессии, чтобы получить отклик \"$y$\" мы складывали все признаки с весами (добавив еще вес для свободного коэффициента):\n",
    "\n",
    "$y = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n$\n",
    "\n",
    "В случае регрессии нас все устраивало. Мы получали в \"$y$\" какое-либо вещественное значение, а в задаче классификации нам нужно получить метку класса - \"+\" или \"-\" (\"+1\" или \"-1\", \"0\" или \"1\"). Для этого можно взять знак от выражения для регрессии:\n",
    "\n",
    "$y(x) = sign(w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n)$ - это и есть формула линейного классификатора.\n",
    "\n",
    "В математике функция $sign(x)$ (знак числа) определена так:   \n",
    "$sign(x) = 1$, если $x > 0,$   \n",
    "$sign(x) = -1$, если $x < 0$,   \n",
    "$sign(x) = 0$, если $x = 0$.\n",
    "\n",
    "Теперь давайте разберемся, какой геометрический смысл у линейного классификатора. \n",
    "\n",
    "$w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n$ - это уравнение гиперплоскости, причем эта плоскость расположена перпендикулярно вектору весов этой модели. Таким образом, эта плоскость делить пространство на два класса и все объекты, которые находятся с одной стороны относит к классу \"+1\", а те которые с другой стороны - к классу \"-1\" (см. левую часть рисунка).\n",
    "\n",
    "<img src=\"linearNonlinear.png\">  \n",
    "\n",
    "\n",
    "Если это можно сделать без ошибок, то обучающая выборка называется *линейно разделимой*. А в случае, если поверхность разделяющая классы уже не является  плоскостью, говорят о нелинейном классификаторе (см. правую часть рисунка).  \n",
    "\n",
    "Если посмотреть на данное разделение в двумерном пространстве, то разделяющая плоскость превратится в линию. Если объект располагается над ней, то алгоритм относит его к классу \"+\", а если под ней, то к классу \"-\". Очевидно, что изображенный на рисунке классификатор ошибается довольно часто.  \n",
    "\n",
    "<img src=\"logreg2.png\" width=60%> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Логистическая регрессия\n",
    "\n",
    "Логистическая регрессия это частный случай линейного классификатора, она относится к методам обучения с учителем. И здесь так же, как и в линейной регрессии, по признаковому описанию объекта (по вектору признаков) предказывается значение отклика (целевой переменной). А именно, вероятность принадлежности объекта к классу. \n",
    "\n",
    "Известно, что все регрессионные модели имеют следующий вид:\n",
    "\n",
    "$y = F(x_1,x_2,...,x_n)$\n",
    "\n",
    "И, теоретически, ничто нам не мешает ее использовать в задаче классификации. Мы можем воспользоваться градиентным спуском и подобрать веса модели в зависимости от признаков и ответа на объектах выборки.\n",
    "Допустим, мы ее применили для классификации в задаче \"Пойдет ли завтра дождь?\": класс \"+\" будет означать, что пойдет, а \"-\", что не пойдет. И линейная регрессия даст ответ, который может быть любым вещественным числом, в диапазоне от $-\\infty$ до $+\\infty$. Получается, что нам нужна функция, которая позволит преобразовать пространство всех вещественных чисел в диапазон от $[0,1]$.\n",
    "\n",
    "Такая функция есть, она принадлежит семейству обобщенных линейных моделей, и называется она \"сигмоида\":\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + \\exp^{-z}}$$\n",
    "\n",
    "$$z = F(x_1,x_2,...,x_n)$$\n",
    "\n",
    "Для одномерного случая: $w_1x + w_0$\n",
    "- значение $w_0$ (свободный коэффициент) определяет положение центра сигмоиды на числовой оси;\n",
    "- а вес при единственном признаке $w_1$ определяет форму сигмоиды.\n",
    "\n",
    "Если $w_1$ положительный, то сигмоида возрастающая, а если отрицательный, то убывающая.\n",
    "\n",
    "<img src=\"sigm.png\"> <img src=\"sigm2.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чем больше значение $w_1$ по модулю, тем больше будет наклон сигмоиды к области ее середины: <img src=\"sigm3.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, логистическая регрессия прогнозирует вероятность отнесения примера к классу \"+\" (при условии, что мы знаем его признаки и веса модели) как сигмоид-преобразование линейной комбинации вектора весов модели \"$w$\" и вектора признаков \"$x$\":\n",
    "\n",
    "$$y = \\frac{1}{1 + \\exp^{-(w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n)}}$$\n",
    "\n",
    "В результате, мы получаем значение \"$y$\" в диапазоне от $[0,1]$ - и это будет вероятность того, что данный объект принадлежит к классу \"+\". А итоговую метку класса мы сможем назначить, сравнивая эту вероятность с 0.5 (центр сигмоиды). Соответственно, класс \"+\" будет при y > 0.5, а класс \"-\" при y <= 0.5\n",
    "\n",
    "Получается, что самое главное в классификации логистической регрессией - подобрать веса $w$ при признаках в показателе степени $e$. Эту задачу можно решить при помощи метода максимального правдоподобия."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Метод максимального правдоподобия\n",
    "\n",
    "Метод максимального правдоподобия (англ. maximum likelihood estimation) в математической статистике — это метод оценивания неизвестного параметра путём максимизации функции правдоподобия. Основан на предположении о том, что вся информация о статистической выборке содержится в функции правдоподобия.\n",
    "\n",
    "Это метод поиска модели, наилучшим образом описывающей обучающую выборку, полученную с некоторым неизвестным распределением. С его помощью можно подобрать такую оценку, при которой вероятность получить имеющиеся данные максимальна. Давайте разберем его на примере.\n",
    "\n",
    "Допустим, по пути из школы домой мы задали один и тот же вопрос 10 случайным людям.  \n",
    "<center><b>\"Нравится ли вам погода сегодня?\"</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{array}{rr} \\hline\n",
    "Респондент &{Ответ } \\\\ \\hline\n",
    "1 &нравится \\\\ \n",
    "2 &нет \\\\ \n",
    "3 &нравится \\\\ \n",
    "4 &нет \\\\ \n",
    "5 &нравится \\\\ \n",
    "6 &нравится \\\\ \n",
    "7 &нравится \\\\ \n",
    "8 &нравится \\\\\n",
    "9 &нет \\\\ \n",
    "10 &нравится \\\\ \n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь у нас есть обучающающая выборка из 10 людей. И мы предполагаем, что вероятность того, что погода сегодня нравится случайному человеку равно некоему значению \"$p$\".\n",
    "\n",
    "\\begin{array}{rr} \\hline\n",
    "Респондент &{Ответ } \\\\ \\hline\n",
    "1 &p \\\\ \n",
    "2 &(1-p) \\\\ \n",
    "3 &p \\\\ \n",
    "4 &(1-p) \\\\ \n",
    "5 &p \\\\ \n",
    "6 &p \\\\ \n",
    "7 &p \\\\ \n",
    "8 &p \\\\\n",
    "9 &(1-p) \\\\ \n",
    "10 &p \\\\ \n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тогда вероятность получить такую же картину как в таблице будет равна произведению вероятностей, так как мы считаем эти события независимыми:  \n",
    "$$p\\cdot (1-p)\\cdot p\\cdot(1-p)\\cdot p\\cdot p\\cdot p\\cdot p\\cdot (1-p)\\cdot p  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Упростив, мы получим функцию:  \n",
    "$$p^7*(1-p)^3$$\n",
    "\n",
    "И теперь подберем такое значение \"$p$\" при котором эта вероятность максимальна.\n",
    "\n",
    "Визуально это означает найти точку на оси $x$, которой соответствует вершина этого графика: <img src=\"p.png\">\n",
    "\n",
    "Как вы знаете, чтобы максимизировать функцию, придется брать ее производную. Производную от суммы брать проще, чем от произведения, поэтому чтобы облегчить себе жизнь, математики решили сначала логарифмировать эту функцию, а потом брать производную.\n",
    "\n",
    "$$L = \\log({p^7(1-p)^3})$$\n",
    "\n",
    "<img src=\"logp.png\">\n",
    "\n",
    "Функцию $p^7*(1-p)^3$ называют <b>функцией правдоподобия</b>, а $\\log({p^7(1-p)^3})$ - <b>логарифмической функцией правдоподобия</b>. Само же значение $p$, при котором и функция правдоподобия, и логарифмическая функция правдоподобия принимают максимальные значения, называют <b>оценкой максимального правдоподобия</b>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, найдем оценку максимального правдоподобия для нашего примера.\n",
    "\n",
    "<b>Шаг 1:</b>   \n",
    "По выборке выводим функцию правдоподобия = $p\\cdot (1-p)\\cdot p\\cdot(1-p)\\cdot p\\cdot p\\cdot p\\cdot p\\cdot (1-p)\\cdot p$  \n",
    "\n",
    "<b>Шаг 2:</b>   \n",
    "Берем логарифм от этой функции и упрощаем ее.  \n",
    "$L = \\log({p^7(1-p)^3}) = \\log{p^7} + \\log{(1-p)^3} = 7\\log{p} + 3log{(1-p)}$  \n",
    "\n",
    "<b>Шаг 3:</b>   \n",
    "Дифференцируем (берем производную по $p$) логарифмическую функцию правдоподобия $L$ и приравниваем к нулю  \n",
    "$7 \\cdot \\frac{1}{p} - 3 \\cdot \\frac{1}{1-p} = 0$\n",
    "\n",
    "Домножаем обе части на $p(1-p)$:\n",
    "\n",
    "$(7\\cdot \\frac{1}{p} - 3 \\cdot\\frac{1}{1-p})\\cdot p(1-p) = 0\\cdot p(1-p)$  \n",
    "$7(1-p) - 3p = 0$  \n",
    "$7-10p = 0$  \n",
    "$p = 0.7$ - и вот она, максимальная оценка правдоподобия!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Домашняя работа</center>\n",
    "Представьте, что на следующий день вы шли из школы в два раза дольше и успели опросить больше, но совсем других случайных людей. И вот какие ответы получили.\n",
    "<center><b>\"Нравится ли вам погода сегодня?\"</b></center>\n",
    "\n",
    "\\begin{array}{rr} \\hline\n",
    "Респондент &{Ответ } \\\\ \\hline\n",
    "1 &да \\\\ \n",
    "2 &нет \\\\ \n",
    "3 &нет \\\\ \n",
    "4 &нет \\\\ \n",
    "5 &да \\\\ \n",
    "6 &нет \\\\ \n",
    "7 &да \\\\ \n",
    "8 &да \\\\\n",
    "9 &нет \\\\ \n",
    "10 &да \\\\ \n",
    "11 &да \\\\ \n",
    "12 &да \\\\ \n",
    "13 &нет \\\\ \n",
    "14 &да \\\\ \n",
    "15 &да \\\\ \n",
    "\\end{array}\n",
    "\n",
    "Рассчитайте максимальную оценку правдоподобия для этого случая."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
