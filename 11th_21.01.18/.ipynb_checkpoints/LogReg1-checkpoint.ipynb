{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Логистическая регрессия</center>\n",
    "\n",
    "#### Линейный классификатор\n",
    "\n",
    "Перед тем, как непосредственно перейти к логистической регрессии, рассмотрим задачу линейной классификации в целом. И начнем с самого простого ее варианта - бинарной классификации. В этом случае мы будем определять к какому из двух классов можно отнести объект.  \n",
    "  \n",
    "Мы уже знакомы с линейной регрессией и линейный классификатор устроен очень похоже. В линейной регрессии, чтобы получить отклик \"$y$\" мы складывали все признаки с весами (добавив еще вес для свободного коэффициента):\n",
    "\n",
    "$y = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n$\n",
    "\n",
    "В случае регрессии нас все устраивало. Мы получали в \"$y$\" какое-либо вещественное значение, а в задаче классификации нам нужно получить метку класса - \"+\" или \"-\" (\"+1\" или \"-1\", \"0\" или \"1\"). Для этого можно взять знак от выражения для регрессии:\n",
    "\n",
    "$y(x) = sign(w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n)$ - это и есть формула линейного классификатора.\n",
    "\n",
    "В математике функция $sign(x)$ (знак числа) определена так:   \n",
    "$sign(x) = 1$, если $x > 0,$   \n",
    "$sign(x) = -1$, если $x < 0$,   \n",
    "$sign(x) = 0$, если $x = 0$.\n",
    "\n",
    "Теперь давайте разберемся, какой геометрический смысл у линейного классификатора. \n",
    "\n",
    "$w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n$ - это уравнение гиперплоскости, причем эта плоскость расположена перпендикулярно вектору весов этой модели. Таким образом, эта плоскость делить пространство на два класса и все объекты, которые находятся с одной стороны относит к классу \"+1\", а те которые с другой стороны - к классу \"-1\" (см. левую часть рисунка).\n",
    "\n",
    "<img src=\"linearNonlinear.png\">  \n",
    "\n",
    "\n",
    "Если это можно сделать без ошибок, то обучающая выборка называется *линейно разделимой*. А в случае, если поверхность разделяющая классы уже не является  плоскостью, говорят о нелинейном классификаторе (см. правую часть рисунка).  \n",
    "\n",
    "Если посмотреть на данное разделение в двумерном пространстве, то разделяющая плоскость превратится в линию. Если объект располагается над ней, то алгоритм относит его к классу \"+\", а если под ней, то к классу \"-\". Очевидно, что изображенный на рисунке классификатор ошибается довольно часто.  \n",
    "\n",
    "<img src=\"logreg2.png\" width=60%> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Логистическая регрессия\n",
    "\n",
    "Логистическая регрессия это частный случай линейного классификатора, она относится к методам обучения с учителем. И здесь так же, как и в линейной регрессии, по признаковому описанию объекта (по вектору признаков) предказывается значение отклика (целевой переменной). А именно, вероятность принадлежности объекта к классу. \n",
    "\n",
    "Известно, что все регрессионные модели имеют следующий вид:\n",
    "\n",
    "$y = F(x_1,x_2,...,x_n)$\n",
    "\n",
    "И, теоретически, ничто нам не мешает ее использовать в задаче классификации. Мы можем воспользоваться градиентным спуском и подобрать веса модели в зависимости от признаков и ответа на объектах выборки.\n",
    "Допустим, мы ее применили для классификации в задаче \"Пойдет ли завтра дождь?\": класс \"+\" будет означать, что пойдет, а \"-\", что не пойдет. И линейная регрессия даст ответ, который может быть любым вещественным числом, в диапазоне от $-\\infty$ до $+\\infty$. Получается, что нам нужна функция, которая позволит преобразовать пространство всех вещественных чисел в диапазон от $[0,1]$.\n",
    "\n",
    "Такая функция есть, она принадлежит семейству обобщенных линейных моделей, и называется она \"сигмоида\":\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + \\exp^{-z}}$$\n",
    "\n",
    "$$z = F(x_1,x_2,...,x_n)$$\n",
    "\n",
    "Для одномерного случая: $w_1x + w_0$\n",
    "- значение $w_0$ (свободный коэффициент) определяет положение центра сигмоиды на числовой оси;\n",
    "- а вес при единственном признаке $w_1$ определяет форму сигмоиды.\n",
    "\n",
    "Если $w_1$ положительный, то сигмоида возрастающая, а если отрицательный, то убывающая.\n",
    "\n",
    "<img src=\"sigm.png\"> <img src=\"sigm2.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чем больше значение $w_1$ по модулю, тем больше будет наклон сигмоиды к области ее середины: <img src=\"sigm3.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, логистическая регрессия прогнозирует вероятность отнесения примера к классу \"+\" (при условии, что мы знаем его признаки и веса модели) как сигмоид-преобразование линейной комбинации вектора весов модели \"$w$\" и вектора признаков \"$x$\":\n",
    "\n",
    "$$y = \\frac{1}{1 + \\exp^{-(w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n)}}$$\n",
    "\n",
    "В результате, мы получаем значение \"$y$\" в диапазоне от $[0,1]$ - и это будет вероятность того, что данный объект принадлежит к классу \"+\". А итоговую метку класса мы сможем назначить, сравнивая эту вероятность с 0.5 (центр сигмоиды). Соответственно, класс \"+\" будет при y > 0.5, а класс \"-\" при y <= 0.5\n",
    "\n",
    "Получается, что самое главное в классификации логистической регрессией - подобрать веса $w$ при признаках в показателе степени $e$. Эту задачу можно решить при помощи метода максимального правдоподобия."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Метод максимального правдоподобия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "говорит о том, что нужно выбрать такую оценку, при которой вероятность получить имеющиеся данные максимальна.\n",
    "\n",
    "берем данные, строим столбец из p - считаем события независимыми и в этом случае берем произведения вероятностей\n",
    "\n",
    "мы получили функцию $p^7*(1-p)^3$ и нам нужно ее максимизировать подбирая параметр p\n",
    "как вы знаете, чтобы максимизировать функцию, нам придется брать ее производную - производную от суммы брать проще, чем от произведения, поэтому чтобы облегчить себе жизнь, математики решили сначала логарифмировать эту функцию, а потом брать производную\n",
    "\n",
    "L(p) = ...\n",
    "\n",
    "берем производную по p\n",
    "\n",
    "чтоб найти экстремум - приравниваем производную к нулю\n",
    "\n",
    "таким образом мы получили оценку максимального правдоподобия, т.е. получили максимальную вероятность получить данное значение на выборке\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
